{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "###Data Processing for GAT-TSNE Algorithm ######\n",
    "################################################\n",
    "##               Heqiao Ruan                  ##\n",
    "##           Department of Statistics,        ##\n",
    "##                 Genome Center,             ##\n",
    "##        University of California, Davis     ##\n",
    "##                 hruan@ucdavis.edu          ##\n",
    "################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_groupdata = fetch_20newsgroups(shuffle = True, random_state = 42, remove = ('header', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lxml import html, etree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Dense, Activation, Embedding, Dropout, Input, RepeatVector, TimeDistributed, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/Rhq/Desktop/UCDAVIS/ECS 271/Project/Data/cora/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "#Cora data:\n",
    "feature_labels = np.genfromtxt('/Users/Rhq/Desktop/UCDAVIS/ECS 271/Project/Data/cora/cora.content', dtype = np.dtype(str))\n",
    "                               \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sp.csr_matrix(feature_labels[:, 1: -1], dtype = np.float32)\n",
    "idx = np.array(feature_labels[:, 0], dtype = np.int32)\n",
    "idx_map = {j: i for i,j in enumerate(idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding the labels:\n",
    "def one_hot_enc(Labels):\n",
    "\t#Encode the labels:\n",
    "\tset_of_labels = set(Labels)\n",
    "\tclass_dict = {c: np.identity(len(set_of_labels))[i, :] for i, c in enumerate(set_of_labels)}\n",
    "\tone_hot_label = np.array(list(map(class_dict.get, Labels)),\n",
    "\t\tdtype = np.int32)\n",
    "\n",
    "\treturn one_hot_label\n",
    "labels = one_hot_enc(feature_labels[:, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct adjacency matrix:\n",
    "edges_0 = np.genfromtxt('/Users/Rhq/Desktop/UCDAVIS/ECS 271/Project/Data/cora/cora.cites', dtype = np.int32)\n",
    "edges = np.array(list(map(idx_map.get, edges_0.flatten())),\n",
    "\tdtype = np.int32).reshape(edges_0.shape)\n",
    "adj_matrix = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "\tshape = (labels.shape[0], labels.shape[0]), dtype = np.float32)\n",
    "adj_matrix = adj_matrix + adj_matrix.T.multiply(adj_matrix.T > adj_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = normalize(adj_matrix + sp.eye(adj_matrix.shape[0]))\n",
    "feature_data = normalize(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gat_arc(Layer):\n",
    "\n",
    "\tdef __init__(self, out_feature, attn_integrate = 'average', dropout = 0.3, activation = 'LeakyRelu', kernel_initializer = 'glorot_uniform', bias_initializer = 'zeros', weight_regularizer = None, bias_regularizer = None, attn_num = 8, sigma = 10):\n",
    "\t\tself.out_feature = out_feature #The number of output features\n",
    "\t\tself.activation  = activations.get(activation) # Activation function.\n",
    "\t\tself.kernel_initializer = initializers.get(kernel_initializer) # Weight initialization\n",
    "\t\tself.bias_initializer = initializers.get(bias_initializer) # Bias initialization\n",
    "\t\tself.weight_regularizer = regularizers.get(weight_regularizer) # Weight regularizer\n",
    "\t\tself.bias_regularizer = regularizers.get(bias_regularizer) # Bias regularizer\n",
    "\t\tself.attn_integrate = attn_integrate # The type of reduction, either average or conca.\n",
    "\t\tself.dropout = dropout # Dropout rate\n",
    "\t\tself.attn_num = attn_num # The number for multi-attention concatenation\n",
    "\t\tself.sigma = sigma # The sigma for the rbf kernel of TSNE.\n",
    "\t\tself.head_kernel = []\n",
    "\t\tself.bias_kernel = []\n",
    "\t\tself.attention_kernel = []\n",
    "\t\t###########\n",
    "\t\tif attn_integrate == 'conca':\n",
    "\t\t\tself.output_dimen = self.out_feature * self.attn_header\n",
    "\t\telse:\n",
    "\t\t\tself.output_dimen = self.out_feature\n",
    "\n",
    "\tdef construct_attention(self, Node_feature, Adj_matrix):\n",
    "\t\t\"\"\"\n",
    "\t\t:Construct the attention Network.\n",
    "\t\t:Generate the node feature representation.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\toutput = []\n",
    "\t\tfor i in range(self.attn_num):\n",
    "\t\t\t#Construct the weights for attention head and neighbors attentions:\n",
    "\t\t\tcurrent_head_kernel = self.add_weight(shape = (p, self.output_feature), initializer = self.kernel_initializer, regularizer = self.weight_regularizer,\n",
    "\t\t\t\tname = 'Head_Number-{}'.format(i))\n",
    "\t\t\tcurrent_bias = self.add_weight(shape = (self.output_feature, ), initializer = self.bias_initializer, regularizer = self.bias_regularizer,\n",
    "\t\t\t\tname = 'Head_Bias-{}'.format(i))\n",
    "\t\t\tself.head_kernel.append(current_head_kernel)\n",
    "\t\t\tself.bias_kernel.append(current_bias)\n",
    "\n",
    "\t\t\tcurrent_attention_itself = self.add_weight(shape = (self.output_feature, 1),\n",
    "\t\t\t\tinitializer = self.kernel_initializer)\n",
    "\t\t\tcurrent_attention_neighbor = self.add_weight(shape = (self.output_feature, 1),\n",
    "\t\t\t\tinitializer = self.kernel_initializer)\n",
    "\t\t\tself.attention_kernel.append([current_attention_itself, current_attention_neighbor])\n",
    "\n",
    "\t\t#Then put the feature onto the assigned tensors:\n",
    "\t\tfor i in range(self.attn_num):\n",
    "\t\t\tcurrent_kernel = self.head_kernel[i]\n",
    "\t\t\tcurrent_attention_kernel = self.attention_kernel[i]\n",
    "\n",
    "\t\t\t#Inputs for the attention kernel:\n",
    "\t\t\tcurrent_feature = K.dot(Node_feature, current_kernel)\n",
    "\n",
    "\t\t\t#Attention mechanism combination:\n",
    "\t\t\tcurrent_attention_itself = K.dot(current_feature, current_attention_kernel[0])\n",
    "\t\t\tcurrent_attention_neighbors = K.dot(current_feature, current_attention_kernel[1])\n",
    "\t\t\tATTE = current_attention_itself + K.transpose(current_attention_neighbors)\n",
    "\n",
    "\t\t\t#lrelu:\n",
    "\t\t\tATTE = LeakyRelu(alpha = 0.25)(ATTE)\n",
    "\n",
    "\t\t\t#Restrict the attention to its neighbors by a mask:\n",
    "\t\t\tMASK = K.exp(Adj_matrix * -10e9) * -10e9\n",
    "\t\t\tATTE += MASK\n",
    "\n",
    "\t\t\t#Get the Attention coefficients:\n",
    "\t\t\tATTE = K.softmax(ATTE)\n",
    "\n",
    "\t\t\t#Apply the dropout to features and attention coefficients:\n",
    "\t\t\tcurrent_feature = Dropout(self.dropout)(current_feature)\n",
    "\t\t\tATTE = Dropout(self.dropout)(current_feature)\n",
    "\n",
    "\t\t\t#Combination for the neighbors' features:\n",
    "\t\t\tNode_feature = K.dot(ATTE, current_feature)\n",
    "\n",
    "\t\t\tNode_feature = K.bias_add(Node_features, self.bias_kernel[i])\n",
    "\n",
    "\t\t#Aggregate the heads together:\n",
    "\t\tif self.attn_integrate == 'conca':\n",
    "\t\t\toutputS = K.concatenate(output)\n",
    "\t\telse:\n",
    "\t\t\toutputS = K.mean(K.stack(output), axis = 0)\n",
    "\n",
    "\t\t#Activate the output:\n",
    "\t\toutputS = self.activation(outputS)\n",
    "\n",
    "\t\treturn outputS\n",
    "\n",
    "\t#Attach a TSNE-loss regularization apart from the classification loss:\n",
    "\t#Modify the perplexity as the neighboring nodes.\n",
    "\tdef TSNE_loss(self, inputs, outputs, Adj_matrix):\n",
    "\t\t#Here we attach the different loss in terms of the tsne loss:\n",
    "\t\tn = inputs.shape[0]\n",
    "\t\tP_table = np.zeros((n, n))\n",
    "\t\tQ_table = np.zeros((n, n))\n",
    "\n",
    "\t\t#Construct the graph similarity measure:\n",
    "\t\tfor i in range(n):\n",
    "\t\t\t#Find all neighbor index:\n",
    "\t\t\tdenom = 0\n",
    "\t\t\tpnumm = []\n",
    "\t\t\tqnumm = []\n",
    "\t\t\tneighbor_index = [i for i, x in enumerate(list(Adj_matrix[i])) if x == 1]\n",
    "\t\t\t#Calculate the P and Q values:\n",
    "\t\t\tfor k in neighbor_index:\n",
    "\t\t\t\t#Calculate the P table:\n",
    "\t\t\t\tpnum = self.compute_p(inputs, i, k, self.sigma)\n",
    "\t\t\t\tpnumm.append(pnum)\n",
    "\t\t\t\tpnumm /= np.sum(pnumm)\n",
    "\n",
    "\t\t\t\t#Calculate the Q table:\n",
    "\t\t\t\tqnum = self.compute_q(outputs, i, k)\n",
    "\t\t\t\tqnumm.append(qnum)\n",
    "\t\t\t\tqnumm /= np.sum(qnumm)\n",
    "\t\t\tP_table[i, neighbor_index] = pnumm\n",
    "\t\t\tQ_table[i, neighbor_index] = qnumm\n",
    "\t\t\n",
    "\t\t#Normalize the P_table:\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tfor j in range(i, n):\n",
    "\t\t\t\ts = P_table[i, j]\n",
    "\t\t\t\tt = P_table[j, i]\n",
    "\t\t\t\tP_table[i, j] = (s + t) / 2\n",
    "\t\t\t\tP_table[j, i] = (s + t) / 2\n",
    "\t\treturn kl_div\n",
    "\n",
    "\tdef compute_p(self, X, i, j):\n",
    "\t\t#Function to calculate the P in the original feature space:\n",
    "\t\tx1 = X[i]\n",
    "\t\tx2 = X[j]\n",
    "\t\tnum = np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n",
    "\t\treturn num\n",
    "\n",
    "\tdef compute_q(self, Y, i, j):\n",
    "\t\t#Function to calculate the Likelihood in the compressed feature space:\n",
    "\t\ty1 = Y[i]\n",
    "\t\ty2 = Y[j]\n",
    "\t\tnum = (1 + np.linalg.norm(y1 - y2) ** 2) ** (-1)\n",
    "\t\treturn num\n",
    "\n",
    "\n",
    "    #Function to calculate the KL_loss:\n",
    "\tdef KL_div(self, Inputs, Q):\n",
    "\t\tkld = 0\n",
    "\t\tn = P.shape[0]\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tfor j in range(n):\n",
    "\t\t\t\tif (P[i, j] != 0) and (Q[i, j] != 0):\n",
    "\t\t\t\t\tkld += P[i, j] * np.log(P[i,j] / Q[i,j])\n",
    "\t\treturn kld\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the T4S1 dataset:\n",
    "import pandas as pd\n",
    "os.chdir('/Users/Rhq/Desktop/UCDAVIS/ECS 271/Project/Data')\n",
    "AA_T4 = pd.read_csv('T4_scRNA.csv', sep = ' ')\n",
    "BB_S1 = pd.read_csv('S1_scRNA.csv', sep = ' ')\n",
    "AA_T4 = np.asarray(AA_T4)\n",
    "BB_S1 = np.asarray(BB_S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(monitor='acc', patience=es_patience)\n",
    "tb_callback = TensorBoard(batch_size=N)\n",
    "mc_callback = ModelCheckpoint('logs/best_model.h5',\n",
    "                              monitor='acc',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_A = AA_T4.shape[0]\n",
    "n_B = BB_S1.shape[0]\n",
    "labels_t4s1= np.zeros((n_A + n_B ,2))\n",
    "labels_t4s1[:n_A,0] = 1\n",
    "labels_t4s1[n_A:(n_A + n_B), 1] = 1\n",
    "labels_t4s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_T4 = kneighbors_graph(AA_T4, n_neighbors = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_S1 = kneighbors_graph(BB_S1, n_neighbors = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_T4S1 = np.concatenate([AA_T4, BB_S1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj_t4s1 = kneighbors_graph(AB_T4S1, n_neighbors = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a model for scRNA-seq data:\n",
    "N = AB_T4S1.shape[0]\n",
    "F = AB_T4S1.shape[1]\n",
    "F_ = 8\n",
    "n_attn_heads = 8\n",
    "dropout_rate = 0.6\n",
    "l2_reg = 1e-3\n",
    "X_in = Input(shape=(F,))\n",
    "n_classes = 2\n",
    "A_in = Input(shape=(N,))\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='elu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "graph_attention_2 = GraphAttention(n_classes,\n",
    "                                   attn_heads=1,\n",
    "                                   attn_heads_reduction='average',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='sigmoid',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout2, A_in])\n",
    "\n",
    "# Build model\n",
    "models = Model(inputs = [X_in, A_in], outputs = graph_attention_1)\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val = np.randint()\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 1851)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_200 (Dropout)           (None, 1851)         0           input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           (None, 4777)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_19 (GraphAttent (None, 64)           118656      dropout_200[0][0]                \n",
      "                                                                 input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_217 (Dropout)           (None, 64)           0           graph_attention_19[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_20 (GraphAttent (None, 2)            134         dropout_217[0][0]                \n",
      "                                                                 input_36[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 118,790\n",
      "Trainable params: 118,790\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 1.8181 - acc: 0.5530\n",
      "Epoch 2/15\n",
      "4777/4777 [==============================] - 6s 1ms/step - loss: 1.4813 - acc: 0.5771\n",
      "Epoch 3/15\n",
      "4777/4777 [==============================] - 5s 1ms/step - loss: 1.8072 - acc: 0.5994\n",
      "Epoch 4/15\n",
      "4777/4777 [==============================] - 5s 965us/step - loss: 2.0717 - acc: 0.4956\n",
      "Epoch 5/15\n",
      "4777/4777 [==============================] - 5s 967us/step - loss: 2.0088 - acc: 0.5583\n",
      "Epoch 6/15\n",
      "4777/4777 [==============================] - 5s 1ms/step - loss: 2.0486 - acc: 0.5066\n",
      "Epoch 7/15\n",
      "4777/4777 [==============================] - 5s 1ms/step - loss: 1.6527 - acc: 0.5778\n",
      "Epoch 8/15\n",
      "4777/4777 [==============================] - 5s 1ms/step - loss: 1.7902 - acc: 0.5736\n",
      "Epoch 9/15\n",
      "4777/4777 [==============================] - 5s 1ms/step - loss: 2.5006 - acc: 0.5254\n",
      "Epoch 10/15\n",
      "4777/4777 [==============================] - 5s 1ms/step - loss: 1.6906 - acc: 0.5806\n",
      "Epoch 11/15\n",
      "4777/4777 [==============================] - 5s 1ms/step - loss: 1.8668 - acc: 0.5256\n",
      "Epoch 12/15\n",
      "4777/4777 [==============================] - 5s 1ms/step - loss: 2.1714 - acc: 0.5282\n",
      "Epoch 13/15\n",
      "4777/4777 [==============================] - 5s 946us/step - loss: 2.1866 - acc: 0.5340\n",
      "Epoch 14/15\n",
      "4777/4777 [==============================] - 5s 972us/step - loss: 1.9169 - acc: 0.5320\n",
      "Epoch 15/15\n",
      "4777/4777 [==============================] - 5s 981us/step - loss: 1.6065 - acc: 0.5480\n"
     ]
    }
   ],
   "source": [
    "#fit the model:\n",
    "learning_rate = 1e-6\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "history2 = model.fit([AB_T4S1, Adj_t4s1], labels_t4s1, epochs = 15, batch_size = N, shuffle = False, callbacks=[es_callback, tb_callback, mc_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('logs/best_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 1851)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_200 (Dropout)           (None, 1851)         0           input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           (None, 4777)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_19 (GraphAttent (None, 64)           118656      dropout_200[0][0]                \n",
      "                                                                 input_36[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 118,656\n",
      "Trainable params: 118,656\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [32,32] vs. [32,4777]\n\t [[{{node graph_attention_19/add_1}} = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](graph_attention_19/leaky_re_lu_87/LeakyRelu, graph_attention_19/mul)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-6e9eaeb06427>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlowdim_repre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAB_T4S1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdj_t4s1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [32,32] vs. [32,4777]\n\t [[{{node graph_attention_19/add_1}} = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](graph_attention_19/leaky_re_lu_87/LeakyRelu, graph_attention_19/mul)]]"
     ]
    }
   ],
   "source": [
    "lowdim_repre = models.predict([AB_T4S1, Adj_t4s1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [32,32] vs. [32,4777]\n\t [[{{node graph_attention_19/add_11}} = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](graph_attention_19/leaky_re_lu_92/LeakyRelu, graph_attention_20/mul)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-3470112192b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m intermediate_layer_model = Model(inputs=model.input,\n\u001b[1;32m      4\u001b[0m                                  outputs=model.get_layer('graph_attention_20').output)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlowdim_repre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAB_T4S1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdj_t4s1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [32,32] vs. [32,4777]\n\t [[{{node graph_attention_19/add_11}} = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](graph_attention_19/leaky_re_lu_92/LeakyRelu, graph_attention_20/mul)]]"
     ]
    }
   ],
   "source": [
    "#Get the low-dimensional representation:\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('graph_attention_20').output)\n",
    "lowdim_repre = intermediate_layer_model.predict([AB_T4S1, Adj_t4s1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 1851)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_200 (Dropout)           (None, 1851)         0           input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           (None, 4777)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_19 (GraphAttent (None, 64)           118656      dropout_200[0][0]                \n",
      "                                                                 input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_217 (Dropout)           (None, 64)           0           graph_attention_19[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_20 (GraphAttent (None, 2)            134         dropout_217[0][0]                \n",
      "                                                                 input_36[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 118,790\n",
      "Trainable params: 118,790\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.input_layer.InputLayer at 0x15159aeb8>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[3].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.input_layer.InputLayer at 0x15159aeb8>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'gat_arc' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-14b7a2349436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m g_atten_1 = gat_arc(out_feature = F_, attn_num = 8,\n\u001b[1;32m      7\u001b[0m                          \u001b[0mattn_integrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'concat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'elu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                          )([X, A_in])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_atten_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mg_atten_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgat_arc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_integrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'average'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0;31m# Handle laying building (weight creating, input spec locking).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'gat_arc' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "F_ = 8\n",
    "X_in = Input(shape = (F_, ))\n",
    "A_in = Input(shape = (N, ))\n",
    "X = Dropout(0.2)(X_in)\n",
    "g_atten_1 = gat_arc(out_feature = F_, attn_num = 8,\n",
    "                         attn_integrate = 'concat', dropout = 0.3, activation = 'elu',\n",
    "                         )([X, A_in])\n",
    "X2 = Dropout(0.2)(g_atten_1)\n",
    "g_atten_2 = gat_arc(out_feature = 7, attn_num = 2, attn_integrate = 'average', activation = 'softmax')\n",
    "\n",
    "model = Model(inputs = [X_in, A_in], outputs = g_atten_2)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "XMAT = kneighbors_graph(X, n_neighbors = 6, mode = 'connectivity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 1., 0., 1., 1., 0., 1., 1., 1.],\n",
       "        [1., 0., 0., 1., 0., 1., 1., 1., 0., 1.],\n",
       "        [1., 0., 0., 1., 1., 0., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 0., 0., 1., 1., 1., 0., 1.],\n",
       "        [1., 0., 1., 1., 0., 1., 1., 0., 1., 0.],\n",
       "        [1., 1., 0., 1., 0., 0., 1., 1., 0., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 0., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 0., 1., 1., 0., 0., 1.],\n",
       "        [1., 0., 1., 1., 1., 0., 0., 1., 0., 1.],\n",
       "        [0., 0., 1., 1., 0., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XMAT.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 F_,\n",
    "                 attn_heads=1,\n",
    "                 attn_heads_reduction='concat',  # {'concat', 'average'}\n",
    "                 dropout_rate=0.5,\n",
    "                 activation='relu',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 attn_kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 attn_kernel_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 attn_kernel_constraint=None,\n",
    "                 **kwargs):\n",
    "        if attn_heads_reduction not in {'concat', 'average'}:\n",
    "            raise ValueError('Possbile reduction methods: concat, average')\n",
    "\n",
    "        self.F_ = F_  # Number of output features (F' in the paper)\n",
    "        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n",
    "        self.attn_heads_reduction = attn_heads_reduction  # Eq. 5 and 6 in the paper\n",
    "        self.dropout_rate = dropout_rate  # Internal dropout rate\n",
    "        self.activation = activations.get(activation)  # Eq. 4 in the paper\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.attn_kernel_initializer = initializers.get(attn_kernel_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.attn_kernel_regularizer = regularizers.get(attn_kernel_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.attn_kernel_constraint = constraints.get(attn_kernel_constraint)\n",
    "        self.supports_masking = False\n",
    "\n",
    "        # Populated by build()\n",
    "        self.kernels = []       # Layer kernels for attention heads\n",
    "        self.biases = []        # Layer biases for attention heads\n",
    "        self.attn_kernels = []  # Attention kernels for attention heads\n",
    "\n",
    "        if attn_heads_reduction == 'concat':\n",
    "            # Output will have shape (..., K * F')\n",
    "            self.output_dim = self.F_ * self.attn_heads\n",
    "        else:\n",
    "            # Output will have shape (..., F')\n",
    "            self.output_dim = self.F_\n",
    "\n",
    "        super(GraphAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        F = input_shape[0][-1]\n",
    "\n",
    "        # Initialize weights for each attention head\n",
    "        for head in range(self.attn_heads):\n",
    "            # Layer kernel\n",
    "            kernel = self.add_weight(shape=(F, self.F_),\n",
    "                                     initializer=self.kernel_initializer,\n",
    "                                     regularizer=self.kernel_regularizer,\n",
    "                                     constraint=self.kernel_constraint,\n",
    "                                     name='kernel_{}'.format(head))\n",
    "            self.kernels.append(kernel)\n",
    "\n",
    "            # # Layer bias\n",
    "            if self.use_bias:\n",
    "                bias = self.add_weight(shape=(self.F_, ),\n",
    "                                       initializer=self.bias_initializer,\n",
    "                                       regularizer=self.bias_regularizer,\n",
    "                                       constraint=self.bias_constraint,\n",
    "                                       name='bias_{}'.format(head))\n",
    "                self.biases.append(bias)\n",
    "\n",
    "            # Attention kernels\n",
    "            attn_kernel_self = self.add_weight(shape=(self.F_, 1),\n",
    "                                               initializer=self.attn_kernel_initializer,\n",
    "                                               regularizer=self.attn_kernel_regularizer,\n",
    "                                               constraint=self.attn_kernel_constraint,\n",
    "                                               name='attn_kernel_self_{}'.format(head),)\n",
    "            attn_kernel_neighs = self.add_weight(shape=(self.F_, 1),\n",
    "                                                 initializer=self.attn_kernel_initializer,\n",
    "                                                 regularizer=self.attn_kernel_regularizer,\n",
    "                                                 constraint=self.attn_kernel_constraint,\n",
    "                                                 name='attn_kernel_neigh_{}'.format(head))\n",
    "            self.attn_kernels.append([attn_kernel_self, attn_kernel_neighs])\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs[0]  # Node features (N x F)\n",
    "        A = inputs[1]  # Adjacency matrix (N x N)\n",
    "\n",
    "        outputs = []\n",
    "        for head in range(self.attn_heads):\n",
    "            kernel = self.kernels[head]  # W in the paper (F x F')\n",
    "            attention_kernel = self.attn_kernels[head]  # Attention kernel a in the paper (2F' x 1)\n",
    "\n",
    "            # Compute inputs to attention network\n",
    "            features = K.dot(X, kernel)  # (N x F')\n",
    "\n",
    "            # Compute feature combinations\n",
    "            # Note: [[a_1], [a_2]]^T [[Wh_i], [Wh_2]] = [a_1]^T [Wh_i] + [a_2]^T [Wh_j]\n",
    "            attn_for_self = K.dot(features, attention_kernel[0])    # (N x 1), [a_1]^T [Wh_i]\n",
    "            attn_for_neighs = K.dot(features, attention_kernel[1])  # (N x 1), [a_2]^T [Wh_j]\n",
    "\n",
    "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "            dense = attn_for_self + K.transpose(attn_for_neighs)  # (N x N) via broadcasting\n",
    "\n",
    "            # Add nonlinearty\n",
    "            dense = LeakyReLU(alpha=0.2)(dense)\n",
    "\n",
    "            # Mask values before activation (Vaswani et al., 2017)\n",
    "            mask = -10e9 * (1.0 - A)\n",
    "            dense += mask\n",
    "\n",
    "            # Apply softmax to get attention coefficients\n",
    "            dense = K.softmax(dense)  # (N x N)\n",
    "\n",
    "            # Apply dropout to features and attention coefficients\n",
    "            dropout_attn = Dropout(self.dropout_rate)(dense)  # (N x N)\n",
    "            dropout_feat = Dropout(self.dropout_rate)(features)  # (N x F')\n",
    "\n",
    "            # Linear combination with neighbors' features\n",
    "            node_features = K.dot(dropout_attn, dropout_feat)  # (N x F')\n",
    "\n",
    "            if self.use_bias:\n",
    "                node_features = K.bias_add(node_features, self.biases[head])\n",
    "\n",
    "            # Add output of attention head to final output\n",
    "            outputs.append(node_features)\n",
    "\n",
    "        # Aggregate the heads' output according to the reduction method\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = K.concatenate(outputs)  # (N x KF')\n",
    "        else:\n",
    "            output = K.mean(K.stack(outputs), axis=0)  # N x F')\n",
    "\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = input_shape[0][0], self.output_dim\n",
    "        return output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    #FILE_PATH = os.path.abspath(__file__)\n",
    "    #DIR_PATH = os.path.dirname(FILE_PATH)\n",
    "    #DATA_PATH = os.path.join(DIR_PATH, 'data/')\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"ind.{}.{}\".format( dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"ind.{}.test.index\".format( dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder),\n",
    "                                    max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + 500)\n",
    "    #idx_test = range(len(y) + 500, len(labels))\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_cit, X_cit, Y_train_cit, Y_val_cit, Y_test_cit, id_train_cit, id_val_cit, id_test_cit = load_data('citeseer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on the \n",
    "np.sum(id_train_cit) + np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/Rhq/Desktop/UCDAVIS/ECS 271/Project/Data/cora')\n",
    "A, X, Y_train, Y_val, Y_test, idx_train, idx_val, idx_test = load_data('citeseer')\n",
    "N = X.shape[0]                # Number of nodes in the graph\n",
    "F = X.shape[1]                # Original feature dimension\n",
    "n_classes = Y_train.shape[1]  # Number of classes\n",
    "F_ = 8                        # Output size of first GraphAttention layer\n",
    "n_attn_heads = 8              # Number of attention heads in first GAT layer\n",
    "dropout_rate = 0.6            # Dropout rate (between and inside GAT layers)\n",
    "l2_reg = 5e-4/2               # Factor for l2 regularization\n",
    "learning_rate = 5e-3          # Learning rate for Adam\n",
    "epochs = 10000                # Number of training epochs\n",
    "es_patience = 100    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in power\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#A, X, Y_train, Y_val, Y_test, idx_train, idx_val, idx_test = load_data('cora')\n",
    "\n",
    "# Parameters\n",
    "N = X.shape[0]                # Number of nodes in the graph\n",
    "F = X.shape[1]                # Original feature dimension\n",
    "n_classes = Y_train.shape[1]  # Number of classes\n",
    "F_ = 16                        # Output size of first GraphAttention layer\n",
    "n_attn_heads = 16              # Number of attention heads in first GAT layer\n",
    "dropout_rate = 0.6            # Dropout rate (between and inside GAT layers)\n",
    "l2_reg = 5e-4/2               # Factor for l2 regularization\n",
    "learning_rate = 5e-3          # Learning rate for Adam\n",
    "epochs = 500          # Number of training epochs\n",
    "es_patience = 100             # Patience fot early stopping\n",
    "\n",
    "# Preprocessing operations\n",
    "X = preprocess_features(X)\n",
    "A = A + np.eye(A.shape[0])  # Add self-loops\n",
    "\n",
    "# Model definition (as per Section 3.3 of the paper)\n",
    "X_in = Input(shape=(F,))\n",
    "A_in = Input(shape=(N,))\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='elu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "graph_attention_2 = GraphAttention(n_classes,\n",
    "                                   attn_heads=1,\n",
    "                                   attn_heads_reduction='average',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='softmax',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout2, A_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_53 (InputLayer)           (None, 3703)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_396 (Dropout)           (None, 3703)         0           input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_54 (InputLayer)           (None, 3327)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_37 (GraphAttent (None, 256)          948736      dropout_396[0][0]                \n",
      "                                                                 input_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_429 (Dropout)           (None, 256)          0           graph_attention_37[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_38 (GraphAttent (None, 6)            1554        dropout_429[0][0]                \n",
      "                                                                 input_54[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 950,290\n",
      "Trainable params: 950,290\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3327 samples, validate on 3327 samples\n",
      "Epoch 1/80\n",
      "3327/3327 [==============================] - 37s 11ms/step - loss: 1.9395 - weighted_acc: 0.1417 - val_loss: 1.9074 - val_weighted_acc: 0.1000\n",
      "Epoch 2/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.9042 - weighted_acc: 0.2000 - val_loss: 1.8859 - val_weighted_acc: 0.0780\n",
      "Epoch 3/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.8773 - weighted_acc: 0.1583 - val_loss: 1.8649 - val_weighted_acc: 0.1060\n",
      "Epoch 4/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.8528 - weighted_acc: 0.2667 - val_loss: 1.8491 - val_weighted_acc: 0.1780\n",
      "Epoch 5/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.8425 - weighted_acc: 0.2083 - val_loss: 1.8390 - val_weighted_acc: 0.1620\n",
      "Epoch 6/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.8318 - weighted_acc: 0.2333 - val_loss: 1.8320 - val_weighted_acc: 0.1800\n",
      "Epoch 7/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.8195 - weighted_acc: 0.2583 - val_loss: 1.8282 - val_weighted_acc: 0.2700\n",
      "Epoch 8/80\n",
      "3327/3327 [==============================] - 15s 4ms/step - loss: 1.8106 - weighted_acc: 0.2917 - val_loss: 1.8264 - val_weighted_acc: 0.2180\n",
      "Epoch 9/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.8139 - weighted_acc: 0.2417 - val_loss: 1.8252 - val_weighted_acc: 0.3240\n",
      "Epoch 10/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.7840 - weighted_acc: 0.3750 - val_loss: 1.8255 - val_weighted_acc: 0.3580\n",
      "Epoch 11/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.8064 - weighted_acc: 0.2583 - val_loss: 1.8259 - val_weighted_acc: 0.3780\n",
      "Epoch 12/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.8187 - weighted_acc: 0.2583 - val_loss: 1.8256 - val_weighted_acc: 0.3360\n",
      "Epoch 13/80\n",
      "3327/3327 [==============================] - 15s 4ms/step - loss: 1.8029 - weighted_acc: 0.2833 - val_loss: 1.8236 - val_weighted_acc: 0.3240\n",
      "Epoch 14/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.7941 - weighted_acc: 0.2833 - val_loss: 1.8203 - val_weighted_acc: 0.3160\n",
      "Epoch 15/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.8036 - weighted_acc: 0.2833 - val_loss: 1.8162 - val_weighted_acc: 0.3780\n",
      "Epoch 16/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.7928 - weighted_acc: 0.3000 - val_loss: 1.8114 - val_weighted_acc: 0.4780\n",
      "Epoch 17/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.8072 - weighted_acc: 0.2583 - val_loss: 1.8051 - val_weighted_acc: 0.4960\n",
      "Epoch 18/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.7792 - weighted_acc: 0.3750 - val_loss: 1.7991 - val_weighted_acc: 0.5020\n",
      "Epoch 19/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7991 - weighted_acc: 0.3417 - val_loss: 1.7934 - val_weighted_acc: 0.5000\n",
      "Epoch 20/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7734 - weighted_acc: 0.3667 - val_loss: 1.7897 - val_weighted_acc: 0.4920\n",
      "Epoch 21/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.7621 - weighted_acc: 0.4250 - val_loss: 1.7869 - val_weighted_acc: 0.4940\n",
      "Epoch 22/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7632 - weighted_acc: 0.3917 - val_loss: 1.7855 - val_weighted_acc: 0.4780\n",
      "Epoch 23/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7675 - weighted_acc: 0.3417 - val_loss: 1.7845 - val_weighted_acc: 0.4800\n",
      "Epoch 24/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7466 - weighted_acc: 0.4000 - val_loss: 1.7836 - val_weighted_acc: 0.4980\n",
      "Epoch 25/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7419 - weighted_acc: 0.4083 - val_loss: 1.7831 - val_weighted_acc: 0.4880\n",
      "Epoch 26/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7514 - weighted_acc: 0.4083 - val_loss: 1.7818 - val_weighted_acc: 0.5420\n",
      "Epoch 27/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7108 - weighted_acc: 0.4583 - val_loss: 1.7813 - val_weighted_acc: 0.5740\n",
      "Epoch 28/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.7345 - weighted_acc: 0.4083 - val_loss: 1.7806 - val_weighted_acc: 0.6120\n",
      "Epoch 29/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.7262 - weighted_acc: 0.4250 - val_loss: 1.7811 - val_weighted_acc: 0.6440\n",
      "Epoch 30/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7124 - weighted_acc: 0.5167 - val_loss: 1.7799 - val_weighted_acc: 0.6280\n",
      "Epoch 31/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.7128 - weighted_acc: 0.4583 - val_loss: 1.7786 - val_weighted_acc: 0.6060\n",
      "Epoch 32/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7336 - weighted_acc: 0.4083 - val_loss: 1.7775 - val_weighted_acc: 0.6240\n",
      "Epoch 33/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.7082 - weighted_acc: 0.3917 - val_loss: 1.7775 - val_weighted_acc: 0.6460\n",
      "Epoch 34/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.7014 - weighted_acc: 0.4000 - val_loss: 1.7772 - val_weighted_acc: 0.6400\n",
      "Epoch 35/80\n",
      "3327/3327 [==============================] - 15s 4ms/step - loss: 1.7284 - weighted_acc: 0.4333 - val_loss: 1.7750 - val_weighted_acc: 0.6520\n",
      "Epoch 36/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.6549 - weighted_acc: 0.4833 - val_loss: 1.7727 - val_weighted_acc: 0.6580\n",
      "Epoch 37/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.7167 - weighted_acc: 0.3917 - val_loss: 1.7696 - val_weighted_acc: 0.6300\n",
      "Epoch 38/80\n",
      "3327/3327 [==============================] - 15s 4ms/step - loss: 1.7060 - weighted_acc: 0.4583 - val_loss: 1.7654 - val_weighted_acc: 0.6160\n",
      "Epoch 39/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.6691 - weighted_acc: 0.4750 - val_loss: 1.7589 - val_weighted_acc: 0.6140\n",
      "Epoch 40/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.6623 - weighted_acc: 0.5167 - val_loss: 1.7526 - val_weighted_acc: 0.6140\n",
      "Epoch 41/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.6584 - weighted_acc: 0.3833 - val_loss: 1.7468 - val_weighted_acc: 0.6340\n",
      "Epoch 42/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.6827 - weighted_acc: 0.4333 - val_loss: 1.7419 - val_weighted_acc: 0.6400\n",
      "Epoch 43/80\n",
      "3327/3327 [==============================] - 11s 3ms/step - loss: 1.6196 - weighted_acc: 0.4583 - val_loss: 1.7388 - val_weighted_acc: 0.6360\n",
      "Epoch 44/80\n",
      "3327/3327 [==============================] - 11s 3ms/step - loss: 1.6897 - weighted_acc: 0.3833 - val_loss: 1.7348 - val_weighted_acc: 0.6360\n",
      "Epoch 45/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.6559 - weighted_acc: 0.4167 - val_loss: 1.7305 - val_weighted_acc: 0.6400\n",
      "Epoch 46/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.6365 - weighted_acc: 0.4083 - val_loss: 1.7264 - val_weighted_acc: 0.6380\n",
      "Epoch 47/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.6457 - weighted_acc: 0.4167 - val_loss: 1.7229 - val_weighted_acc: 0.6440\n",
      "Epoch 48/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.6396 - weighted_acc: 0.4250 - val_loss: 1.7209 - val_weighted_acc: 0.6380\n",
      "Epoch 49/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.6162 - weighted_acc: 0.4417 - val_loss: 1.7222 - val_weighted_acc: 0.6420\n",
      "Epoch 50/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.6551 - weighted_acc: 0.4000 - val_loss: 1.7244 - val_weighted_acc: 0.6540\n",
      "Epoch 51/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.5872 - weighted_acc: 0.4667 - val_loss: 1.7274 - val_weighted_acc: 0.6380\n",
      "Epoch 52/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.6165 - weighted_acc: 0.4333 - val_loss: 1.7271 - val_weighted_acc: 0.6400\n",
      "Epoch 53/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.6787 - weighted_acc: 0.3833 - val_loss: 1.7259 - val_weighted_acc: 0.6440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.5967 - weighted_acc: 0.4667 - val_loss: 1.7241 - val_weighted_acc: 0.6260\n",
      "Epoch 55/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.5909 - weighted_acc: 0.4917 - val_loss: 1.7235 - val_weighted_acc: 0.6160\n",
      "Epoch 56/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.6100 - weighted_acc: 0.4917 - val_loss: 1.7212 - val_weighted_acc: 0.6220\n",
      "Epoch 57/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.5482 - weighted_acc: 0.5250 - val_loss: 1.7176 - val_weighted_acc: 0.6220\n",
      "Epoch 58/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.6554 - weighted_acc: 0.4500 - val_loss: 1.7125 - val_weighted_acc: 0.6420\n",
      "Epoch 59/80\n",
      "3327/3327 [==============================] - 15s 5ms/step - loss: 1.6142 - weighted_acc: 0.4417 - val_loss: 1.7056 - val_weighted_acc: 0.6760\n",
      "Epoch 60/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.5539 - weighted_acc: 0.4917 - val_loss: 1.7009 - val_weighted_acc: 0.6980\n",
      "Epoch 61/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.6248 - weighted_acc: 0.3917 - val_loss: 1.6982 - val_weighted_acc: 0.6780\n",
      "Epoch 62/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.5892 - weighted_acc: 0.4833 - val_loss: 1.6931 - val_weighted_acc: 0.6560\n",
      "Epoch 63/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.5868 - weighted_acc: 0.4917 - val_loss: 1.6904 - val_weighted_acc: 0.6340\n",
      "Epoch 64/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.6339 - weighted_acc: 0.4833 - val_loss: 1.6863 - val_weighted_acc: 0.6460\n",
      "Epoch 65/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.5892 - weighted_acc: 0.4583 - val_loss: 1.6826 - val_weighted_acc: 0.6520\n",
      "Epoch 66/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.6010 - weighted_acc: 0.4583 - val_loss: 1.6777 - val_weighted_acc: 0.6740\n",
      "Epoch 67/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.5862 - weighted_acc: 0.4083 - val_loss: 1.6745 - val_weighted_acc: 0.7060\n",
      "Epoch 68/80\n",
      "3327/3327 [==============================] - 15s 5ms/step - loss: 1.5585 - weighted_acc: 0.4917 - val_loss: 1.6705 - val_weighted_acc: 0.7200\n",
      "Epoch 69/80\n",
      "3327/3327 [==============================] - 16s 5ms/step - loss: 1.5212 - weighted_acc: 0.4583 - val_loss: 1.6679 - val_weighted_acc: 0.7300\n",
      "Epoch 70/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.5881 - weighted_acc: 0.4500 - val_loss: 1.6659 - val_weighted_acc: 0.7220\n",
      "Epoch 71/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.5525 - weighted_acc: 0.5167 - val_loss: 1.6641 - val_weighted_acc: 0.7220\n",
      "Epoch 72/80\n",
      "3327/3327 [==============================] - 11s 3ms/step - loss: 1.5284 - weighted_acc: 0.4833 - val_loss: 1.6633 - val_weighted_acc: 0.7000\n",
      "Epoch 73/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.4927 - weighted_acc: 0.4583 - val_loss: 1.6641 - val_weighted_acc: 0.7020\n",
      "Epoch 74/80\n",
      "3327/3327 [==============================] - 14s 4ms/step - loss: 1.5587 - weighted_acc: 0.5167 - val_loss: 1.6647 - val_weighted_acc: 0.7040\n",
      "Epoch 75/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.4809 - weighted_acc: 0.5667 - val_loss: 1.6674 - val_weighted_acc: 0.7020\n",
      "Epoch 76/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.5251 - weighted_acc: 0.4750 - val_loss: 1.6695 - val_weighted_acc: 0.6980\n",
      "Epoch 77/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.5731 - weighted_acc: 0.4917 - val_loss: 1.6697 - val_weighted_acc: 0.6980\n",
      "Epoch 78/80\n",
      "3327/3327 [==============================] - 13s 4ms/step - loss: 1.6528 - weighted_acc: 0.3917 - val_loss: 1.6708 - val_weighted_acc: 0.6780\n",
      "Epoch 79/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.5216 - weighted_acc: 0.4583 - val_loss: 1.6710 - val_weighted_acc: 0.6780\n",
      "Epoch 80/80\n",
      "3327/3327 [==============================] - 12s 4ms/step - loss: 1.5261 - weighted_acc: 0.4917 - val_loss: 1.6712 - val_weighted_acc: 0.6700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6704590320587158, 0.7359996438026428]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_callback = EarlyStopping(monitor='val_weighted_acc', patience=es_patience)\n",
    "tb_callback = TensorBoard(batch_size=N)\n",
    "mc_callback = ModelCheckpoint('logs/best_model.h5',\n",
    "                              monitor='val_weighted_acc',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True)\n",
    "\n",
    "# Train model\n",
    "validation_data = ([X, A], Y_val, idx_val)\n",
    "model.fit([X, A],\n",
    "          Y_train,\n",
    "          sample_weight=idx_train,\n",
    "          epochs = 80,\n",
    "          batch_size=N,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[es_callback, tb_callback, mc_callback])\n",
    "\n",
    "# Load best model\n",
    "model.load_weights('logs/best_model.h5')\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = model.evaluate([X, A],\n",
    "                              Y_test,\n",
    "                              sample_weight=idx_test,\n",
    "                              batch_size=N,\n",
    "                              verbose=0)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record the \n",
    "#8 -> 0.8160\n",
    "#16 -> 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_300 (Dropout)           (None, 1433)         0           input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           (None, 2708)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_29 (GraphAttent (None, 64)           91904       dropout_300[0][0]                \n",
      "                                                                 input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_317 (Dropout)           (None, 64)           0           graph_attention_29[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_30 (GraphAttent (None, 7)            469         dropout_317[0][0]                \n",
      "                                                                 input_46[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 92,373\n",
      "Trainable params: 92,373\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When feeding symbolic tensors to a model, we expect thetensors to have a static batch size. Got tensor with shape: (None, 1433)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-cb46943bb37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Shuffling data means shuffling the whole graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m           callbacks=[es_callback,  mc_callback])\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;34m'When feeding symbolic tensors to a model, we expect the'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;34m'tensors to have a static batch size. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 'Got tensor with shape: %s' % str(shape))\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: When feeding symbolic tensors to a model, we expect thetensors to have a static batch size. Got tensor with shape: (None, 1433)"
     ]
    }
   ],
   "source": [
    "F = X.shape[1]\n",
    "F_ = 8\n",
    "n_attn_heads = 8\n",
    "dropout_rate = 0.4\n",
    "l2_reg = 5e-4\n",
    "X_in = Input(shape=(F,))\n",
    "n_classes = 7\n",
    "A_in = Input(shape=(N,))\n",
    "N = X.shape[0]\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='elu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "graph_attention_2 = GraphAttention(F_,\n",
    "                                   attn_heads=1,\n",
    "                                   attn_heads_reduction='average',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='softmax',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout2, A_in])\n",
    "gf2 = \n",
    "# Build model\n",
    "models = Model(inputs = [X_in, A_in], outputs = graph_attention_1)\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)\n",
    "learning_rate = 5e-4\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()\n",
    "validation_data = ([X_in, A_in], Y_train, idx_val)\n",
    "history1 = model.fit([X_in, A_in],\n",
    "          Y_val,\n",
    "          sample_weight=idx_train,\n",
    "          epochs = 30, \n",
    "          batch_size = N,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[es_callback,  mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-8090bb055e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeract\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_cit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_cit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'graph_attention_13'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keract/keract.py\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(model, x, layer_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0minput_layer_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0minput_layer_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'input_'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mactivations_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mactivations_inputs_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_layer_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keract/keract.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(model, nodes_to_evaluate, x, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymb_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_to_evaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weight_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 array_vals.append(\n\u001b[1;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2655\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from keract import get_activations\n",
    "activations = get_activations(model, [X_cit, A_cit], 'graph_attention_13')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare to the TSNE visualization of the graph features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = model.evaluate([X, A],\n",
    "                              Y_test,\n",
    "                              sample_weight=idx_test,\n",
    "                              batch_size=N,\n",
    "                              verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4395214319229126, 0.9008708596229553]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros(labels.shape)\n",
    "y_val = np.zeros(labels.shape)\n",
    "y_test = np.zeros(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the original model:\n",
    "n_label = labels.shape[1]\n",
    "n_atten_head = 16\n",
    "learning_rate = 5e-3\n",
    "epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = X.shape[1]\n",
    "F_ = 8\n",
    "n_attn_heads = 8\n",
    "dropout_rate = 0.4\n",
    "l2_reg = 5e-4\n",
    "X_in = Input(shape=(F,))\n",
    "n_classes = 7\n",
    "A_in = Input(shape=(N,))\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='elu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "graph_attention_2 = GraphAttention(n_classes,\n",
    "                                   attn_heads=1,\n",
    "                                   attn_heads_reduction='average',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='softmax',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout2, A_in])\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct model to calculate the novel loss function:\n",
    "def TSNE_model(sigma):\n",
    "    Feature_in = Input(shape = (F, ))\n",
    "    Data_in =\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "#Simulate a fully-connected KNN Graph on top of the Single-cell Genomic Data:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
