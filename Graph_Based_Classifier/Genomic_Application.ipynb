{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/ruanhq/Desktop/Davis_PhD_Study/UCDAVIS/ECS 271/Project/Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply the GAT classifier on the T4S1 data:\n",
    "#Combine the T4(AA) and S1(BB) together to build a classifier.\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "AA_T4 = pd.read_csv('T4_scRNA.csv', sep = ' ')\n",
    "BB_S1 = pd.read_csv('S1_scRNA.csv', sep = ' ')\n",
    "AA_T4 = np.asarray(AA_T4)\n",
    "BB_S1 = np.asarray(BB_S1)\n",
    "n_A = AA_T4.shape[0]\n",
    "n_B = BB_S1.shape[0]\n",
    "labels_t4s1= np.zeros((n_A + n_B ,2))\n",
    "labels_t4s1[:n_A,0] = 1\n",
    "labels_t4s1[n_A:(n_A + n_B), 1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/Rhq/Desktop/UCDAVIS/ECS 271/Project/Data/cora')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "#Construct the Adjacency matrix and the whole genomic data:\n",
    "AB_T4S1 = np.concatenate([AA_T4, BB_S1])\n",
    "Adj_t4s1 = kneighbors_graph(AB_T4S1, n_neighbors = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate another graph topology:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lxml import html, etree\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Layer, Dropout, LeakyReLU\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Dense, Activation, Embedding, Dropout, Input, RepeatVector, TimeDistributed, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import activations, constraints, initializers, regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 F_,\n",
    "                 attn_heads=1,\n",
    "                 attn_heads_reduction='concat',  # {'concat', 'average'}\n",
    "                 dropout_rate=0.5,\n",
    "                 activation='relu',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 attn_kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 attn_kernel_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 attn_kernel_constraint=None,\n",
    "                 **kwargs):\n",
    "        if attn_heads_reduction not in {'concat', 'average'}:\n",
    "            raise ValueError('Possbile reduction methods: concat, average')\n",
    "\n",
    "        self.F_ = F_  # Number of output features (F' in the paper)\n",
    "        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n",
    "        self.attn_heads_reduction = attn_heads_reduction  # Eq. 5 and 6 in the paper\n",
    "        self.dropout_rate = dropout_rate  # Internal dropout rate\n",
    "        self.activation = activations.get(activation)  # Eq. 4 in the paper\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.attn_kernel_initializer = initializers.get(attn_kernel_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.attn_kernel_regularizer = regularizers.get(attn_kernel_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.attn_kernel_constraint = constraints.get(attn_kernel_constraint)\n",
    "        self.supports_masking = False\n",
    "\n",
    "        # Populated by build()\n",
    "        self.kernels = []       # Layer kernels for attention heads\n",
    "        self.biases = []        # Layer biases for attention heads\n",
    "        self.attn_kernels = []  # Attention kernels for attention heads\n",
    "\n",
    "        if attn_heads_reduction == 'concat':\n",
    "            # Output will have shape (..., K * F')\n",
    "            self.output_dim = self.F_ * self.attn_heads\n",
    "        else:\n",
    "            # Output will have shape (..., F')\n",
    "            self.output_dim = self.F_\n",
    "\n",
    "        super(GraphAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        F = input_shape[0][-1]\n",
    "\n",
    "        # Initialize weights for each attention head\n",
    "        for head in range(self.attn_heads):\n",
    "            # Layer kernel\n",
    "            kernel = self.add_weight(shape=(F, self.F_),\n",
    "                                     initializer=self.kernel_initializer,\n",
    "                                     regularizer=self.kernel_regularizer,\n",
    "                                     constraint=self.kernel_constraint,\n",
    "                                     name='kernel_{}'.format(head))\n",
    "            self.kernels.append(kernel)\n",
    "\n",
    "            # # Layer bias\n",
    "            if self.use_bias:\n",
    "                bias = self.add_weight(shape=(self.F_, ),\n",
    "                                       initializer=self.bias_initializer,\n",
    "                                       regularizer=self.bias_regularizer,\n",
    "                                       constraint=self.bias_constraint,\n",
    "                                       name='bias_{}'.format(head))\n",
    "                self.biases.append(bias)\n",
    "\n",
    "            # Attention kernels\n",
    "            attn_kernel_self = self.add_weight(shape=(self.F_, 1),\n",
    "                                               initializer=self.attn_kernel_initializer,\n",
    "                                               regularizer=self.attn_kernel_regularizer,\n",
    "                                               constraint=self.attn_kernel_constraint,\n",
    "                                               name='attn_kernel_self_{}'.format(head),)\n",
    "            attn_kernel_neighs = self.add_weight(shape=(self.F_, 1),\n",
    "                                                 initializer=self.attn_kernel_initializer,\n",
    "                                                 regularizer=self.attn_kernel_regularizer,\n",
    "                                                 constraint=self.attn_kernel_constraint,\n",
    "                                                 name='attn_kernel_neigh_{}'.format(head))\n",
    "            self.attn_kernels.append([attn_kernel_self, attn_kernel_neighs])\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs[0]  # Node features (N x F)\n",
    "        A = inputs[1]  # Adjacency matrix (N x N)\n",
    "\n",
    "        outputs = []\n",
    "        for head in range(self.attn_heads):\n",
    "            kernel = self.kernels[head]  # W in the paper (F x F')\n",
    "            attention_kernel = self.attn_kernels[head]  # Attention kernel a in the paper (2F' x 1)\n",
    "\n",
    "            # Compute inputs to attention network\n",
    "            features = K.dot(X, kernel)  # (N x F')\n",
    "\n",
    "            # Compute feature combinations\n",
    "            # Note: [[a_1], [a_2]]^T [[Wh_i], [Wh_2]] = [a_1]^T [Wh_i] + [a_2]^T [Wh_j]\n",
    "            attn_for_self = K.dot(features, attention_kernel[0])    # (N x 1), [a_1]^T [Wh_i]\n",
    "            attn_for_neighs = K.dot(features, attention_kernel[1])  # (N x 1), [a_2]^T [Wh_j]\n",
    "\n",
    "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "            dense = attn_for_self + K.transpose(attn_for_neighs)  # (N x N) via broadcasting\n",
    "\n",
    "            # Add nonlinearty\n",
    "            dense = LeakyReLU(alpha=0.2)(dense)\n",
    "\n",
    "            # Mask values before activation (Vaswani et al., 2017)\n",
    "            mask = -10e9 * (1.0 - A)\n",
    "            dense += mask\n",
    "\n",
    "            # Apply softmax to get attention coefficients\n",
    "            dense = K.softmax(dense)  # (N x N)\n",
    "\n",
    "            # Apply dropout to features and attention coefficients\n",
    "            dropout_attn = Dropout(self.dropout_rate)(dense)  # (N x N)\n",
    "            dropout_feat = Dropout(self.dropout_rate)(features)  # (N x F')\n",
    "\n",
    "            # Linear combination with neighbors' features\n",
    "            node_features = K.dot(dropout_attn, dropout_feat)  # (N x F')\n",
    "\n",
    "            if self.use_bias:\n",
    "                node_features = K.bias_add(node_features, self.biases[head])\n",
    "\n",
    "            # Add output of attention head to final output\n",
    "            outputs.append(node_features)\n",
    "\n",
    "        # Aggregate the heads' output according to the reduction method\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = K.concatenate(outputs)  # (N x KF')\n",
    "        else:\n",
    "            output = K.mean(K.stack(outputs), axis=0)  # N x F')\n",
    "\n",
    "        output = self.activation(output)\n",
    "        return output, dense\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = input_shape[0][0], self.output_dim\n",
    "        return output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a GAT classifier model:\n",
    "#Examine the attention coefficients:\n",
    "N = AB_T4S1.shape[0]\n",
    "F = AB_T4S1.shape[1]\n",
    "F_ = 6\n",
    "n_attn_heads = 6\n",
    "dropout_rate = 0.4\n",
    "l2_reg = 1e-3\n",
    "X_in = Input(shape=(F,))\n",
    "A_in = Input(shape=(N,))\n",
    "n_classes = 2\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='elu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "AS1, ds1 = Dense(32, activation = 'relu')(graph_attention_1)\n",
    "graph_attention_2 = GraphAttention(8,\n",
    "                                   attn_heads=1,\n",
    "                                   attn_heads_reduction='average',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='relu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([AS1, A_in])\n",
    "learned_labels, dense2 = Dense(n_classes, activation = 'softmax')(graph_attention_2)\n",
    "# Build model\n",
    "model = Model(inputs = [X_in, A_in], outputs = learned_labels)\n",
    "models = Model(inputs = [X_in, A_in], outputs = graph_attention_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 1851)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_491 (Dropout)           (None, 1851)         0           input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           (None, 4777)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_26 (GraphAttent (None, 36)           66744       dropout_491[0][0]                \n",
      "                                                                 input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           1184        graph_attention_26[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_27 (GraphAttent (None, 8)            280         dense_9[0][0]                    \n",
      "                                                                 input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2)            18          graph_attention_27[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 68,226\n",
      "Trainable params: 68,226\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "es_callback = EarlyStopping(monitor='weighted_acc', patience=100)\n",
    "mc_callback = ModelCheckpoint('logs/best_model.h5',\n",
    "                              monitor='weighted_acc',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True)\n",
    "learning_rate = 1e-3\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4777/4777 [==============================] - 53s 11ms/step - loss: 1.8449 - acc: 0.4726\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `weighted_acc` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with weighted_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4777/4777 [==============================] - 8s 2ms/step - loss: 1.3243 - acc: 0.4875\n",
      "Epoch 3/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 1.0030 - acc: 0.5099\n",
      "Epoch 4/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.9572 - acc: 0.5102\n",
      "Epoch 5/30\n",
      "4777/4777 [==============================] - 7s 2ms/step - loss: 0.8303 - acc: 0.5876\n",
      "Epoch 6/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.7483 - acc: 0.5361\n",
      "Epoch 7/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.7115 - acc: 0.6473\n",
      "Epoch 8/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.6323 - acc: 0.7934\n",
      "Epoch 9/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.5534 - acc: 0.7909\n",
      "Epoch 10/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.5061 - acc: 0.8752\n",
      "Epoch 11/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.4886 - acc: 0.9066\n",
      "Epoch 12/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.4879 - acc: 0.8851\n",
      "Epoch 13/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.4609 - acc: 0.9246\n",
      "Epoch 14/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.4493 - acc: 0.9177\n",
      "Epoch 15/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.4322 - acc: 0.9586\n",
      "Epoch 16/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.4186 - acc: 0.9663\n",
      "Epoch 17/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.4255 - acc: 0.9514\n",
      "Epoch 18/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.4315 - acc: 0.9577\n",
      "Epoch 19/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.3922 - acc: 0.9734\n",
      "Epoch 20/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.3838 - acc: 0.9722\n",
      "Epoch 21/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.3698 - acc: 0.9749\n",
      "Epoch 22/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.3698 - acc: 0.9738\n",
      "Epoch 23/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.3543 - acc: 0.9828\n",
      "Epoch 24/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.3522 - acc: 0.9751\n",
      "Epoch 25/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.3262 - acc: 0.9870\n",
      "Epoch 26/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.3330 - acc: 0.9826\n",
      "Epoch 27/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.3287 - acc: 0.9853\n",
      "Epoch 28/30\n",
      "4777/4777 [==============================] - 9s 2ms/step - loss: 0.3170 - acc: 0.9885\n",
      "Epoch 29/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.3092 - acc: 0.9864\n",
      "Epoch 30/30\n",
      "4777/4777 [==============================] - 8s 2ms/step - loss: 0.2986 - acc: 0.9849\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit([AB_T4S1, Adj_t4s1],\n",
    "          labels_t4s1,\n",
    "          epochs = 30, \n",
    "          batch_size=N,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[es_callback,  mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's visualize the data\n",
    "Low_dim_representation2 = models.predict([AB_T4S1, Adj_t4s1], batch_size = N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the csv:\n",
    "np.savetxt('GAT_feature_T4S1_50.csv', Low_dim_representation, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the TSNE:\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components = 2, perplexity = 40)\n",
    "GAT_tsne_2 = tsne.fit_transform(Low_dim_representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4777, 8)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We get a 8-dimensional manifold:\n",
    "Low_dim_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the attention coefficient:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"GAT_feature_T4S1_50.csv\", GAT_tsne_2, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For another data:\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file = 'CLASF.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second application for the NKT cell dataset:\n",
    "os.chdir('/Users/Rhq/Desktop/UCDAVIS/ECS 271/Project/Data')\n",
    "clus_info = np.asarray(pd.read_csv('clust_info.csv', sep = ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "label2 = to_categorical(clus_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 4)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2 = label2[:, 1:]\n",
    "label2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119425.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Adj_t4s1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "NK_expr = pd.read_csv('NK_hvg.csv', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NK_expr = np.asarray(NK_expr)\n",
    "n_A = 50\n",
    "n_B = 50\n",
    "n_C = 50\n",
    "n_D = 53\n",
    "labels_nk= np.zeros((n_A + n_B + n_C + n_D ,4))\n",
    "labels_nk[:n_A,0] = 1\n",
    "labels_nk[n_A:(n_A + n_B), 1] = 1\n",
    "labels_nk[(n_A + n_B):(n_A + n_B + n_C), 1] = 1\n",
    "labels_nk[(n_A + n_B + n_C):(n_A + n_B + n_C + n_D), 1] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It only have 203 samples so we design a super simple architecture:\n",
    "Adj_NK = kneighbors_graph(NK_expr, n_neighbors = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a model:\n",
    "N = NK_expr.shape[0]\n",
    "F = NK_expr.shape[1]\n",
    "F_ = 2\n",
    "n_attn_heads = 2\n",
    "dropout_rate = 0.2\n",
    "l2_reg = 1e-3\n",
    "X_in = Input(shape=(F,))\n",
    "A_in = Input(shape=(N,))\n",
    "n_classes = 4\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='elu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "AS1 = Dense(2, activation = 'relu')(graph_attention_1)\n",
    "graph_attention_2 = GraphAttention(F_,\n",
    "                                   attn_heads=1,\n",
    "                                   attn_heads_reduction='average',\n",
    "                                   dropout_rate=dropout_rate,\n",
    "                                   activation='relu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg))([AS1, A_in])\n",
    "learned_labels = Dense(n_classes, activation = 'softmax')(graph_attention_2)\n",
    "# Build model\n",
    "model = Model(inputs = [X_in, A_in], outputs = learned_labels)\n",
    "models = Model(inputs = [X_in, A_in], outputs = graph_attention_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           (None, 2000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_542 (Dropout)           (None, 2000)         0           input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           (None, 203)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_36 (GraphAttent (None, 4)            8012        dropout_542[0][0]                \n",
      "                                                                 input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 2)            10          graph_attention_36[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_37 (GraphAttent (None, 2)            10          dense_19[0][0]                   \n",
      "                                                                 input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 4)            12          graph_attention_37[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 8,044\n",
      "Trainable params: 8,044\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#es_callback = EarlyStopping(monitor='weighted_acc', patience=100)\n",
    "mc_callback = ModelCheckpoint('logs/best_model2.h5',\n",
    "                              monitor='weighted_acc',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = Adam(lr= 1e-6)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "203/203 [==============================] - 37s 180ms/step - loss: 3.6283 - acc: 0.7241\n",
      "Epoch 2/30\n",
      "203/203 [==============================] - 0s 115us/step - loss: 2.8141 - acc: 0.6355\n",
      "Epoch 3/30\n",
      "203/203 [==============================] - 0s 246us/step - loss: 2.7668 - acc: 0.7537\n",
      "Epoch 4/30\n",
      "203/203 [==============================] - 0s 353us/step - loss: 3.2662 - acc: 0.7537\n",
      "Epoch 5/30\n",
      "203/203 [==============================] - 0s 178us/step - loss: 3.7022 - acc: 0.7537\n",
      "Epoch 6/30\n",
      "203/203 [==============================] - 0s 165us/step - loss: 3.5582 - acc: 0.7488\n",
      "Epoch 7/30\n",
      "203/203 [==============================] - 0s 266us/step - loss: 2.6422 - acc: 0.7537\n",
      "Epoch 8/30\n",
      "203/203 [==============================] - 0s 127us/step - loss: 3.9180 - acc: 0.7537\n",
      "Epoch 9/30\n",
      "203/203 [==============================] - 0s 157us/step - loss: 2.7027 - acc: 0.4089\n",
      "Epoch 10/30\n",
      "203/203 [==============================] - 0s 196us/step - loss: 3.6283 - acc: 0.7537\n",
      "Epoch 11/30\n",
      "203/203 [==============================] - 0s 115us/step - loss: 3.2927 - acc: 0.7537\n",
      "Epoch 12/30\n",
      "203/203 [==============================] - 0s 145us/step - loss: 3.3320 - acc: 0.7537\n",
      "Epoch 13/30\n",
      "203/203 [==============================] - 0s 142us/step - loss: 3.2531 - acc: 0.7537\n",
      "Epoch 14/30\n",
      "203/203 [==============================] - 0s 162us/step - loss: 3.0750 - acc: 0.7537\n",
      "Epoch 15/30\n",
      "203/203 [==============================] - 0s 486us/step - loss: 3.7261 - acc: 0.7340\n",
      "Epoch 16/30\n",
      "203/203 [==============================] - 0s 125us/step - loss: 2.0008 - acc: 0.7389\n",
      "Epoch 17/30\n",
      "203/203 [==============================] - 0s 473us/step - loss: 3.4515 - acc: 0.7537\n",
      "Epoch 18/30\n",
      "203/203 [==============================] - 0s 198us/step - loss: 3.5787 - acc: 0.7537\n",
      "Epoch 19/30\n",
      "203/203 [==============================] - 0s 169us/step - loss: 2.0436 - acc: 0.7537\n",
      "Epoch 20/30\n",
      "203/203 [==============================] - 0s 253us/step - loss: 3.2953 - acc: 0.7537\n",
      "Epoch 21/30\n",
      "203/203 [==============================] - 0s 205us/step - loss: 3.5514 - acc: 0.7537\n",
      "Epoch 22/30\n",
      "203/203 [==============================] - 0s 252us/step - loss: 2.2946 - acc: 0.7537\n",
      "Epoch 23/30\n",
      "203/203 [==============================] - 0s 441us/step - loss: 3.4430 - acc: 0.7389\n",
      "Epoch 24/30\n",
      "203/203 [==============================] - 0s 205us/step - loss: 3.8870 - acc: 0.7537\n",
      "Epoch 25/30\n",
      "203/203 [==============================] - 0s 123us/step - loss: 3.1476 - acc: 0.7537\n",
      "Epoch 26/30\n",
      "203/203 [==============================] - 0s 130us/step - loss: 3.2884 - acc: 0.7537\n",
      "Epoch 27/30\n",
      "203/203 [==============================] - 0s 153us/step - loss: 3.5803 - acc: 0.7537\n",
      "Epoch 28/30\n",
      "203/203 [==============================] - 0s 826us/step - loss: 3.7879 - acc: 0.7537\n",
      "Epoch 29/30\n",
      "203/203 [==============================] - 0s 135us/step - loss: 2.6045 - acc: 0.7537\n",
      "Epoch 30/30\n",
      "203/203 [==============================] - 0s 134us/step - loss: 3.4114 - acc: 0.7537\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit([NK_expr, Adj_NK], labels_nk,\n",
    "          epochs = 30, \n",
    "          batch_size=N,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[es_callback,  mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "Low_dim_repre = models.predict([NK_expr, Adj_NK], batch_size = N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('NK_2dim.csv', Low_dim_repre, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file = 'NK_arc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the purity issue for attention coefficients:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
